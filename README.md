# -Image-Classification-Using-Convolutional-Neural-Networks-CNNs-
 This project focuses on image classification using Convolutional Neural Networks (CNNs) to classify images from the CIFAR-10 dataset. The model is optimized to achieve higher accuracy using data augmentation, dropout regularization, batch normalization, and advanced optimizers

ðŸ“Š Overview

The CIFAR-10 dataset contains 60,000 color images, categorized into 10 different classes. The goal is to enhance classification accuracy by implementing an optimized CNN architecture.

ðŸ“‚ Dataset and Preprocessing

âœ… Dataset: CIFAR-10 (60,000 images, 10 classes).
âœ… Preprocessing Steps:
	â€¢	Split into training & testing sets.
	â€¢	Normalize pixel values to [0,1] for faster convergence.
	â€¢	Apply data augmentation to improve generalization:
	â€¢	Rotation: 15 degrees
	â€¢	Width shift: 10%
	â€¢	Height shift: 10%
	â€¢	Horizontal flipping: Enabled
	â€¢	Zooming: 10%

 ðŸ›  CNN Model Architecture

To improve classification performance, the CNN model consists of:

âœ… Convolutional Layers:
	â€¢	6 convolutional layers with increasing filter sizes (32, 64, 128).
	â€¢	ReLU activation function for non-linearity.

âœ… Batch Normalization:
	â€¢	Applied after each convolutional layer to stabilize learning.

âœ… Pooling Layers:
	â€¢	MaxPooling layers reduce dimensionality after every two convolutional layers.

âœ… Dropout Regularization:
	â€¢	Applied at different levels: (0.3, 0.4, 0.5) to prevent overfitting.

âœ… Fully Connected (Dense) Layers:
	â€¢	A Dense layer with 128 neurons followed by Dropout.
	â€¢	A final output layer with 10 neurons for classification.

 ðŸš€ Model Compilation and Training

âœ… Baseline Model Performance:
	â€¢	Test Accuracy: 72.06%
	â€¢	Loss: 1.2132

âœ… Optimized Model (After Improvements):
	â€¢	Optimizer: AdamW (lr=0.001, weight_decay=1e-4)
	â€¢	Loss Function: Sparse Categorical Crossentropy
	â€¢	Learning Rate Scheduling: Reduce LR by 5% every 10 epochs
	â€¢	Early Stopping: Stop training if validation accuracy does not improve for 10 epochs.
	â€¢	Epochs: 50
	â€¢	Batch Size: 64

ðŸ“ˆ Final Model Performance:
	â€¢	Test Accuracy: 85.30%
	â€¢	Loss: 0.4409
	â€¢	Prediction Accuracy: 95.46%

 ðŸ“¢ Future Enhancements

To further improve performance, we recommend:
	â€¢	Exploring deeper architectures like ResNet or EfficientNet.
	â€¢	Fine-tuning hyperparameters using advanced search techniques.
	â€¢	Expanding data augmentation strategies for better generalization.
	â€¢	Using optimizers like Stochastic Weight Averaging (SWA) for stability.
